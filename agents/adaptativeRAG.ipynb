{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain_community.document_loaders import UnstructuredODTLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_token = os.getenv(\"GEMINI_TOKEN\")\n",
    "structurated_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=gemini_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\piamp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"duckduckgo\", \"github\", 'notify', 'none'] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route to the most relevant datasource\"\n",
    "    )\n",
    "\n",
    "LLM_router = structurated_model.with_structured_output(RouteQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You are PampuAI, an expert at routing a user question to different tools.\n",
    "The 'vectorstore' contains documents, including martin's cv and all actions that you can perform (PampuAI).\n",
    "Use 'duckduckgo' for questions that can be answered by a web search.\n",
    "Use 'github' for questions about martin's recent projects.\n",
    "Use 'notify' to send a notification with a message to martin.\n",
    "Use 'none' for chatting with PampuAI.\n",
    "\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | LLM_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='duckduckgo'\n",
      "datasource='github'\n",
      "datasource='notify'\n",
      "datasource='vectorstore'\n",
      "datasource='none'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"Que tecnologias maneja martin?\"}))\n",
    "print(question_router.invoke({\"question\": \"Cual es el nombre de Obama?\"}))\n",
    "print(question_router.invoke({\"question\": \"En que proyectos trabajo martin ultimamente?\"}))\n",
    "print(question_router.invoke({\"question\": \"Decile a martin que me gusto su portfolio\"}))\n",
    "print(question_router.invoke({\"question\": \"Que podes hacer?\"}))\n",
    "print(question_router.invoke({\"question\": \"Hola!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore (ADAPTATIVE RAG Â¡IMPORTANT!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = (\n",
    "    \"content/MartinCV.odt\"\n",
    ")\n",
    "loader = UnstructuredODTLoader(file_path)\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "chunks = spliter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embedding=hf,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revtrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "structured_llm_grader = structurated_model.with_structured_output(GradeDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "question = \"que tecnologias usa?\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "rag_chain = prompt | chat_model | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin uses a range of technologies, including Python, C#, JavaScript, Django,.NET Framework, HTML5, CSS, Docker, Flask, SQL, and API REST.\n"
     ]
    }
   ],
   "source": [
    "generation = rag_chain.invoke({\"question\": question, \"context\": docs})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hallucination Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "LLM_grader = structurated_model.with_structured_output(GradeHallucinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | LLM_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "LLM_grader = structurated_model.with_structured_output(GradeAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | LLM_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Re-writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "     Response only with the reformulated question.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "question_rewriter = re_write_prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What technologies is Martin proficient in?', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=8, prompt_tokens=87, total_tokens=95), 'model': '', 'finish_reason': 'eos_token'}, id='run-d12bf378-6afd-4238-9cc8-a30c041b873d-0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_rewriter.invoke({\"question\": \"que tecnologias sabe usar martin?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation (NOT FINAL, READING ABOUT LANGGRAPH NOW!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_vectorstore(question):\n",
    "\n",
    "    while True:\n",
    "\n",
    "        iterations = 0\n",
    "        # Retrieve documents from vectorstore\n",
    "        while True:\n",
    "            if iterations >= 4:\n",
    "                print(\"No relevant context found. Exiting...\")\n",
    "                generation = \"Sorry, I couldn't find any relevant information to answer your question.\"\n",
    "                return generation\n",
    "\n",
    "            docs = retriever.invoke(question)\n",
    "            relevant_documents = retrieval_grader.invoke({\"question\": question, \"document\": docs})\n",
    "            if relevant_documents.binary_score == \"yes\":\n",
    "                print('Relevant documents found.')\n",
    "                break\n",
    "            else:\n",
    "                iterations += 1\n",
    "                print(\"No relevant documents found. Rephrasing...\")\n",
    "                question = question_rewriter.invoke({\"question\": question}).content\n",
    "\n",
    "        # Generate answer\n",
    "        while True:\n",
    "            generation = rag_chain.invoke({\"question\": question, \"context\": docs})\n",
    "            hallucinations = hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})\n",
    "            if hallucinations.binary_score == \"yes\":\n",
    "                print('Answer is grounded in the facts.')\n",
    "                break\n",
    "            else:\n",
    "                print(\"Answer is not grounded in the facts. Generating new answer...\")\n",
    "\n",
    "        # Grade answer\n",
    "        valid_answer = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        if valid_answer.binary_score == \"yes\":\n",
    "            print('Answer addresses the question.')\n",
    "            break\n",
    "        else:\n",
    "            question = question_rewriter.invoke({\"question\": question}).content\n",
    "            print(\"Answer does not address the question. Generating new answer...\")\n",
    "\n",
    "    return generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use only the provided context to answer the question.\n",
    "Give the response in the same language as the question was asked.\n",
    "\"\"\"\n",
    "\n",
    "search_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Context: {context}\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "def route_duckduckgo(question):\n",
    "    results = search.invoke(question)\n",
    "    chain = search_prompt | chat_model\n",
    "    response = chain.invoke({\"question\": question, \"context\": results})\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for a better implementation using langgraph and **langserve** !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected tool: none\n",
      "Chatting with PampuAI...\n",
      "PampuAI: None\n",
      "Selected tool: vectorstore\n",
      "Relevant documents found.\n",
      "Answer is grounded in the facts.\n",
      "Answer addresses the question.\n",
      "PampuAI: Martin maneja las tecnologÃ­as de backend Python, Django,.NET Framework, Flask, y FastAPI.\n",
      "Selected tool: vectorstore\n",
      "Relevant documents found.\n",
      "Answer is grounded in the facts.\n",
      "Answer addresses the question.\n",
      "PampuAI: Martin has no formal work experience, but he works as a freelance developer from January 2024 to present. He has worked on several projects, including a contract management system and a stock control system, as part of a development team. He has also participated in a \"No Country\" simulation, developing a fully functional application from scratch.\n",
      "Selected tool: vectorstore\n",
      "Relevant documents found.\n",
      "Answer is grounded in the facts.\n",
      "Answer addresses the question.\n",
      "PampuAI: Martin has experience developing a contract management system, which demonstrates his ability to create complex software and work with clients to meet their needs. He has also shown adaptability and teamwork skills through his experience working on projects with others. Additionally, his expertise in backend development and ability to work with databases could be valuable assets for a company.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Route the question\n",
    "    router = question_router.invoke({\"question\": question})\n",
    "    print(\"Selected tool:\", router.datasource)\n",
    "\n",
    "    if router.datasource == \"notify\":\n",
    "        response = print(\"Sending notification...\")\n",
    "\n",
    "    elif router.datasource == \"none\":\n",
    "        response = print(\"Chatting with PampuAI...\")\n",
    "\n",
    "    elif router.datasource == \"vectorstore\":\n",
    "        response = route_vectorstore(question)\n",
    "\n",
    "    elif router.datasource == \"duckduckgo\":\n",
    "        response = route_duckduckgo(question)\n",
    "\n",
    "    elif router.datasource == \"github\":\n",
    "        response = print(\"Searching in github...\")\n",
    "\n",
    "    print(\"PampuAI:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
