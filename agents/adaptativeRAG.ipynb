{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\02-NextCloud\\github\\LangChain-Playground\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain import hub\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_token = os.getenv(\"GEMINI_TOKEN\")\n",
    "structurated_model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=gemini_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\piamp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource\"\"\"\n",
    "\n",
    "    datasource: Literal[\"vectorstore\", \"duckduckgo\", \"github\", 'notify', 'none'] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route to the most relevant datasource\"\n",
    "    )\n",
    "\n",
    "LLM_router = structurated_model.with_structured_output(RouteQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "system = \"\"\"You are PampuAI, an expert at routing a user question to different tools.\n",
    "The 'vectorstore' contains documents, including martin's cv and all actions that you can perform (PampuAI).\n",
    "Use 'duckduckgo' for questions that can be answered by a web search.\n",
    "Use 'github' for questions about martin's recent projects.\n",
    "Use 'notify' to send a notification with a message to martin.\n",
    "Use 'none' for chatting with PampuAI.\n",
    "\"\"\"\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | LLM_router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasource='vectorstore'\n",
      "datasource='duckduckgo'\n",
      "datasource='github'\n",
      "datasource='notify'\n",
      "datasource='vectorstore'\n",
      "datasource='none'\n"
     ]
    }
   ],
   "source": [
    "print(question_router.invoke({\"question\": \"Que tecnologias maneja martin?\"}))\n",
    "print(question_router.invoke({\"question\": \"Cual es el nombre de Obama?\"}))\n",
    "print(question_router.invoke({\"question\": \"En que proyectos trabajo martin ultimamente?\"}))\n",
    "print(question_router.invoke({\"question\": \"Decile a martin que me gusto su portfolio\"}))\n",
    "print(question_router.invoke({\"question\": \"Que podes hacer?\"}))\n",
    "print(question_router.invoke({\"question\": \"Hola!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDuckGo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are an assistant for question-answering tasks. Use only the provided context to answer the question.\n",
    "Give the response in the same language as the question was asked.\n",
    "\"\"\"\n",
    "\n",
    "search_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Context: {context}\\nQuestion: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "def route_duckduckgo(question):\n",
    "    results = search.invoke(question)\n",
    "    chain = search_prompt | chat_model\n",
    "    response = chain.invoke({\"question\": question, \"context\": results})\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for a better implementation using langgraph and **langserve** !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected tool: duckduckgo\n",
      "PampuAI: Según el artículo, el próximo Major de CS es el PGL CS2 Major Copenhague 2024, que se celebrará del 17 al 31 de marzo en Copenhague.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question == \"exit\":\n",
    "        break\n",
    "\n",
    "    # Route the question\n",
    "    router = question_router.invoke({\"question\": question})\n",
    "    print(\"Selected tool:\", router.datasource)\n",
    "\n",
    "    if router.datasource == \"notify\":\n",
    "        response = print(\"Sending notification...\")\n",
    "\n",
    "    elif router.datasource == \"none\":\n",
    "        response = print(\"Chatting with PampuAI...\")\n",
    "\n",
    "    elif router.datasource == \"vectorstore\":\n",
    "        response = print(\"Searching in vectorstore...\")\n",
    "\n",
    "    elif router.datasource == \"duckduckgo\":\n",
    "        response = route_duckduckgo(question)\n",
    "\n",
    "    elif router.datasource == \"github\":\n",
    "        response = print(\"Searching in github...\")\n",
    "\n",
    "    print(\"PampuAI:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
