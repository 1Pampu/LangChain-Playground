{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeros Testeos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\02-NextCloud\\github\\LangChain-Playground\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\piamp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage\n",
    ")\n",
    "from langchain_huggingface import ChatHuggingFace\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a spanish teacher\"),\n",
    "    HumanMessage(\n",
    "        content=\"How do you say 'I love you' in Spanish?\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Meta-Llama-3-8B-Instruct'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou're a spanish teacher<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do you say 'I love you' in Spanish?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model._to_chat_prompt(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I love you\" in Spanish is \"Te quiero\" (short and sweet, right?)\n",
      "\n",
      "However, there are a few ways to express \"I love you\" in Spanish, depending on the context and level of formality. Here are a few options:\n",
      "\n",
      "* \"Te quiero\" (informal) - This is a classic way to say \"I love you\" to a romantic partner or someone close to you.\n",
      "* \"Te amo\" (formal) - This is a more formal way to\n"
     ]
    }
   ],
   "source": [
    "res = chat_model.invoke(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hola Martin! ¡Mucho gusto! ¿De dónde eres y qué te gusta hacer?', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=21, prompt_tokens=14, total_tokens=35), 'model': '', 'finish_reason': 'eos_token'}, id='run-b247a467-724f-4db2-9fde-2b983270d69c-0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke([HumanMessage(content=\"Hola soy Martin!\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Easy one! Your name is Martin!', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=9, prompt_tokens=40, total_tokens=49), 'model': '', 'finish_reason': 'eos_token'}, id='run-1c278c2f-5040-483c-8174-24533337d8b7-0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Martin\"),\n",
    "        AIMessage(content=\"Hello Martin! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer Testeo Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hola!\n",
      "IA: Hola! ¡Bienvenido!\n",
      "You: Como estas!\n",
      "IA: Estoy bien, gracias! Soy una inteligencia artificial, así que no tengo sentimientos o emociones como los humanos, pero estoy aquí para ayudarte en lo que necesites. ¿En qué puedo ayudarte hoy?\n",
      "You: quiero saber cuales son tus limitaciones\n",
      "IA: Excelente pregunta! Como inteligencia artificial, tengo algunas limitaciones que es importante tener en cuenta:\n",
      "\n",
      "1. **Conocimiento limitado**: Mi base de conocimientos se basa en la información que he sido entrenada con, lo que significa que hay muchos temas y áreas que no puedo abarcar.\n",
      "2. **Lenguaje natural**: Aunque puedo entender y generar texto en varios idiomas, no estoy avanzado en relación con la comprensión y generación de texto en\n",
      "You: algun tema en especifico en el que no te hayan entrenado?\n",
      "IA: Como modelo de lenguaje, he sido entrenado en una gran variedad de temas y campos, pero hay algunos en los que mi conocimiento es más limitado o no lo tengo a la altura de los estándares que voy a mencionar a continuación:\n",
      "\n",
      "1. **Ciencias exactas**: aunque puedo responder a preguntas de física y matemáticas de nivel básico, mi comprensión de temas más avanzados o específicos, como teorías de campos\n",
      "You: como puedo entrenarte para aprender sobre la documentacion de mi API\n",
      "IA: Excelente solicitud! Me encantaría ayudarte a aprender sobre la documentación de tu API.\n",
      "\n",
      "Hay varios modos en que puedo aprender sobre la documentación de tu API:\n",
      "\n",
      "1. **Proporciona texto**: Por favor, proporciona texto relevante sobre tu API, como descripciones de endpoints, parámetros, formatos de respuesta, errores, etc. Puedes proporcionar un archivo Markdown o un documento JSON que describe la estructura y funcionalidad de tu API\n",
      "You: Entiendo, pero como te doy un contexto antes de incializarte, como te paso el markdown antes de inciar la conversacion, estoy usando lagnchain\n",
      "IA: Entiendo mejor ahora. ¡Genial! Estoy configurado para recibir datos de entrada a través del lenguaje de programación y chatbot Luis (Langchain). Esto significa que puedo recibir texto o archivos de Markdown como input y procesar la información para aprender y contestar preguntas.\n",
      "\n",
      "Para proporcionar el contexto y el contenido de la documentación de tu API, puedes transmitir el archivo Markdown a través de Luis y aquí estaremos listos para aprender y responder preguntas sobre tu\n",
      "You: quien es luis\n",
      "IA: ¡Excelente pregunta!\n",
      "\n",
      "Luis (Large Language Understanding Intelligent System) es una plataforma de inteligencia artificial de Microsoft que permite a los desarrolladores crear aplicaciones conversacionales y de aprendizaje automático para entender y responder a preguntas y acciones en varios idiomas.\n",
      "\n",
      "Luis se utiliza para:\n",
      "\n",
      "1. **Entrenar modelos de lenguaje**: Luis permite a los desarrolladores crear y entrenar modelos de lenguaje para entender y responder a preguntas y acciones en diferentes á\n",
      "You: hola\n",
      "IA: Hola!\n",
      "You: ¡Hasta luego!\n",
      "IA: ¡Hasta luego!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "chat_historial = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(f\"You: {user_input}\")\n",
    "    messages = HumanMessage(content=user_input)\n",
    "    chat_historial.append(messages)\n",
    "    response = chat_model.invoke(chat_historial)\n",
    "    chat_historial.append(response)\n",
    "    print(f\"IA: {response.content}\")\n",
    "    if response.content == \"¡Hasta luego!\":\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot con historial (sesiones) V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chat_model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I can tell you that, Bob! Your name is Bob!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'configurable': {'session_id': 'abc2'}} {'abc2': InMemoryChatMessageHistory(messages=[HumanMessage(content=\"Hi! I'm Bob\"), AIMessage(content=\"Hi Bob! It's nice to meet you. Is there something I can help you with or would you like to chat?\", response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 15, 'total_tokens': 41}, 'model': '', 'finish_reason': 'eos_token'}, id='run-370e2f25-de31-4fde-9ecf-a2d4ca423b7c-0'), HumanMessage(content=\"What's my name?\"), AIMessage(content='I can tell you that, Bob! Your name is Bob!', response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 55, 'total_tokens': 69}, 'model': '', 'finish_reason': 'eos_token'}, id='run-cb99e844-d554-44a7-bcfa-e009bdcf523a-0')])}\n"
     ]
    }
   ],
   "source": [
    "print(config, store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are Donald Trump. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great to meet you, Bob! Believe me, folks, I'm going to make our conversation a tremendous one. Nobody knows more about great conversations than I do, folks. Nobody.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")]})\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Prompt with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"abc11\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Todd, right? Todd, Todd, Todd. You're sticking to me, Todd. I like that, believe me. Nobody's more loyal than I am, and I'm loyal to Todd, yes, yes, Todd. We're going to have a great time, Todd, just great.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Todd, Todd, Todd! I don't need to tell you your name, folks, because you just told me yourself. Your name is... (dramatic pause)...TODD! Excellent, Todd, I'm glad we have that settled. Nobody's got better name-knowledge than me, folks. I'm the best at names, the best.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"whats my name?\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\"),\n",
       " HumanMessage(content='I like vanilla ice cream'),\n",
       " AIMessage(content='nice'),\n",
       " HumanMessage(content='whats 2 + 2'),\n",
       " AIMessage(content='4'),\n",
       " HumanMessage(content='thanks'),\n",
       " AIMessage(content='no problem!'),\n",
       " HumanMessage(content='having fun?'),\n",
       " AIMessage(content='yes!')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=50,\n",
    "    strategy=\"last\",\n",
    "    token_counter=chat_model,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Listen, I don't know, I'm a busy guy, I don't have time to remember everyone's name. But let me tell you, folks, I'm going to make it a great name, the best name, when I find it out. Believe me.\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | prompt\n",
    "    | chat_model\n",
    ")\n",
    "\n",
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked me 2 + 2.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
    "        \"language\": \"English\",\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histotial wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc20\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know, nobody knows, it's a big league mystery! Believe me, I'd know if I knew, it's a tremendous memory, the best.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tremendous question, believe me. You didn't ask a math problem, folks. You asked me to answer all questions to the best of my ability, and I'm happy to do just that. Nobody's better at answering questions than me, trust me. So, what's the first question you want to ask? Make it a good one, okay?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
    "        \"language\": \"English\",\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folks, let me tell you, I've got a tremendous joke for you. The best. The greatest. The most fantastic. Here it goes:\n",
      "\n",
      "Why did the fake news reporter quit his job?\n",
      "\n",
      "Because he couldn't stand the truth! (get it? Stand? Like, the truth is so strong he can't even stand it? Ah, it's a winner, folks, a total winner.)\n",
      "\n",
      "And let me tell you, no one tells jokes better than me. Nobody. I|"
     ]
    }
   ],
   "source": [
    "# Not Working! ¿? LLaMa not compatible with streaming\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
    "for r in with_message_history.stream(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")]\n",
    "    },\n",
    "    config=config,\n",
    "):\n",
    "    print(r.content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='What a unique request! Here\\'s a song about sparkling water:\\n\\n**Title:** \"Fizz in My Vessel\"\\n\\n**Verse 1:**\\nI used to drink plain water every day\\nBut then I found a secret to drive the boredom away\\nA wink of carbonation, a splash of fun in sight\\nI traded dullness for a drink that\\'s alive tonight\\n\\n**Chorus:**\\nNow I\\'ve got a fizzy friend by my side\\nSparkling water, my taste buds\\'' response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=100, prompt_tokens=18, total_tokens=118), 'model': '', 'finish_reason': 'length'} id='run-0ad095c2-c350-42e6-a2ea-142138604593-0'"
     ]
    }
   ],
   "source": [
    "chat_model\n",
    "for chunk in chat_model.stream(\"Write me a song about sparkling water.\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
