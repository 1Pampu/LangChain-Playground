{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este archivo contiene los modelos que utilizo normalmente para mis proyectos con AI, a continuacion la lista de limitaciones y funciones de cada uno para poder elegir el apropiado segun la situacion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\02-NextCloud\\github\\LangChain-Playground\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "google_token = os.getenv(\"GEMINI_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\piamp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "models.append(model)\n",
    "names.append(\"Meta-Llama-3-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\piamp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "models.append(model)\n",
    "names.append(\"Meta-Llama-3-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\piamp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "models.append(model)\n",
    "names.append(\"Meta-Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to C:\\Users\\piamp\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    "    huggingfacehub_api_token=hf_token\n",
    ")\n",
    "model = ChatHuggingFace(llm=llm)\n",
    "models.append(model)\n",
    "names.append(\"Meta-Llama-3.1-70B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=google_token)\n",
    "models.append(model)\n",
    "names.append(\"Gemini-1.5-Flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key=google_token)\n",
    "models.append(model)\n",
    "names.append(\"Gemini-1.5-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\", google_api_key=google_token)\n",
    "models.append(model)\n",
    "names.append(\"Gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def test_structurated_output(model):\n",
    "    try:\n",
    "        model_g = model.with_structured_output(GradeDocuments)\n",
    "        chain = grade_prompt | model_g\n",
    "        response = chain.invoke({\"document\": \"Martin sabe usar python\", \"question\": \"Martin usa python?\"})\n",
    "        print(response)\n",
    "        if response.binary_score == \"yes\":\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Llama-3-8B-Instruct :\n",
      "Bot: content=\"I'm just an AI, I don't have feelings or emotions like humans do, so I don't have a personal way of being. I exist to provide information and assist with tasks to the best of my ability! How can I help you today?\" response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=52, prompt_tokens=16, total_tokens=68), 'model': '', 'finish_reason': 'eos_token'} id='run-5f87ed26-5870-4308-9a4b-1bd00686c4dd-0'\n",
      "Structurated Output False\n",
      "\n",
      "Meta-Llama-3-70B-Instruct :\n",
      "REQUIRES SUBSCRIPTION\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\02-NextCloud\\github\\LangChain-Playground\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:897: UserWarning: Tools are not supported by the model. This is due to the model not been served by a Text-Generation-Inference server. The provided tool parameters will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structurated Output False\n",
      "\n",
      "Meta-Llama-3.1-8B-Instruct :\n",
      "REQUIRES SUBSCRIPTION\n",
      "Structurated Output False\n",
      "\n",
      "Meta-Llama-3.1-70B-Instruct :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\02-NextCloud\\github\\LangChain-Playground\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:897: UserWarning: Tools are not supported by the model. This is due to the model not been served by a Text-Generation-Inference server. The provided tool parameters will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUIRES SUBSCRIPTION\n",
      "Structurated Output False\n",
      "\n",
      "Gemini-1.5-Flash :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\02-NextCloud\\github\\LangChain-Playground\\.venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:897: UserWarning: Tools are not supported by the model. This is due to the model not been served by a Text-Generation-Inference server. The provided tool parameters will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: content=\"I am an AI language model, so I don't have feelings or experiences like humans do. However, I am here to assist you with any questions or tasks you may have! How can I help you today? \\n\" response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-9e74107c-1df2-4bee-b4fa-ec7c080ea1f1-0' usage_metadata={'input_tokens': 7, 'output_tokens': 44, 'total_tokens': 51}\n",
      "binary_score='yes'\n",
      "Structurated Output True\n",
      "\n",
      "Gemini-1.5-pro :\n",
      "Bot: content=\"I am an AI language model, so I don't have feelings like humans do. However, I'm here and ready to assist you! How can I help you today? \\n\" response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-9c43f3a4-34ce-4bb3-a173-c32e7a22d45c-0' usage_metadata={'input_tokens': 7, 'output_tokens': 37, 'total_tokens': 44}\n",
      "binary_score='yes'\n",
      "Structurated Output True\n",
      "\n",
      "Gemini-1.0-pro :\n",
      "Bot: content='I am well, thank you. How are you doing today?' response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]} id='run-2a4d0eaa-8753-4ee7-b7fd-6ecc44b27d20-0' usage_metadata={'input_tokens': 7, 'output_tokens': 13, 'total_tokens': 20}\n",
      "Structurated Output False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i= 0\n",
    "for model in models:\n",
    "    print(names[i], ':')\n",
    "    try:\n",
    "        print('Bot:', model.invoke(\"Hello, how are you?\"))\n",
    "    except Exception as e:\n",
    "        print('REQUIRES SUBSCRIPTION')\n",
    "    response = test_structurated_output(model)\n",
    "    print('Structurated Output', response)\n",
    "    print('')\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
